// std::tensor (v0.9 GPU core)

native::import "enkai_tensor" ::
    fn enkai_tensor_device(spec: String) -> Int
    fn enkai_tensor_free(handle: Int) -> Int
    fn enkai_tensor_device_free(handle: Int) -> Int
    fn enkai_tensor_randn(shape_json: String, dtype: String, device: Int) -> Int
    fn enkai_tensor_zeros(shape_json: String, dtype: String, device: Int) -> Int
    fn enkai_tensor_matmul(a: Int, b: Int) -> Int
    fn enkai_tensor_add(a: Int, b: Int) -> Int
    fn enkai_tensor_mul(a: Int, b: Int) -> Int
    fn enkai_tensor_reshape(x: Int, shape_json: String) -> Int
    fn enkai_tensor_transpose(x: Int, dim0: Int, dim1: Int) -> Int
    fn enkai_tensor_concat(handles_json: String, dim: Int) -> Int
    fn enkai_tensor_sum(x: Int, dim: Int, keepdim: Int) -> Int
    fn enkai_tensor_mean(x: Int, dim: Int, keepdim: Int) -> Int
    fn enkai_tensor_softmax(x: Int, dim: Int) -> Int
    fn enkai_tensor_masked_softmax(x: Int, mask: Int, dim: Int, mask_type: Int) -> Int
    fn enkai_tensor_masked_softmax_backward(grad_output: Int, output: Int, mask: Int, dim: Int) -> Int
    fn enkai_tensor_relu(x: Int) -> Int
    fn enkai_tensor_sigmoid(x: Int) -> Int
    fn enkai_tensor_dropout(x: Int, p: Float, train: Int) -> Int
    fn enkai_tensor_slice(x: Int, dim: Int, start: Int, end: Int, step: Int) -> Int
    fn enkai_tensor_view(x: Int, shape_json: String) -> Int
    fn enkai_tensor_layernorm(x: Int, w: Int, b: Int, eps: Float) -> Int
    fn enkai_tensor_layernorm_backward(x: Int, w: Int, b: Int, eps: Float, grad_out: Int) -> String
    fn enkai_tensor_embedding(w: Int, ids: Int) -> Int
    fn enkai_tensor_linear(x: Int, w: Int, b: Int) -> Int
    fn enkai_tensor_gelu(x: Int) -> Int
    fn enkai_tensor_to_device(x: Int, device: Int) -> Int
    fn enkai_tensor_to_dtype(x: Int, dtype: String) -> Int
    fn enkai_tensor_shape(x: Int) -> String
    fn enkai_tensor_cross_entropy(logits: Int, targets: Int) -> Int
    fn enkai_tensor_backward(loss: Int) -> Int
    fn enkai_tensor_require_grad(handle: Int) -> Int
    fn enkai_tensor_zero_grad(handle: Int) -> Int
    fn enkai_tensor_zero_grad_multi(handles_json: String) -> Int
    fn enkai_tensor_adamw_step(param: Int, grad: Int, state: Int, lr: Float, beta1: Float, beta2: Float, eps: Float, wd: Float) -> Int
    fn enkai_tensor_adamw_step_multi(params_json: String, grads_json: String, state: Int, lr: Float, beta1: Float, beta2: Float, eps: Float, wd: Float) -> Int
    fn enkai_tensor_save_sharded(dir: String, param: Int, opt_state: Int, meta_json: String) -> Int
    fn enkai_tensor_save_sharded_multi(dir: String, params_json: String, opt_state: Int, meta_json: String) -> Int
    fn enkai_tensor_load_sharded(dir: String) -> Int
    fn enkai_tensor_load_sharded_params(dir: String) -> String
    fn enkai_tensor_load_sharded_opt(dir: String) -> Int
    fn enkai_tensor_load_sharded_opt_multi(dir: String, params_json: String) -> Int
    fn enkai_tensor_load_sharded_meta(dir: String) -> String
::

pub fn device(spec: String) -> Device ::
    return enkai_tensor_device(spec)
::

pub fn randn(shape, dtype: DType, device: Device) -> Tensor ::
    return enkai_tensor_randn(json.stringify(shape), dtype, device)
::

pub fn zeros(shape, dtype: DType, device: Device) -> Tensor ::
    return enkai_tensor_zeros(json.stringify(shape), dtype, device)
::

pub fn matmul(a: Tensor, b: Tensor) -> Tensor ::
    return enkai_tensor_matmul(a, b)
::

pub fn add(a: Tensor, b: Tensor) -> Tensor ::
    return enkai_tensor_add(a, b)
::

pub fn mul(a: Tensor, b: Tensor) -> Tensor ::
    return enkai_tensor_mul(a, b)
::

pub fn reshape(x: Tensor, shape) -> Tensor ::
    return enkai_tensor_reshape(x, json.stringify(shape))
::

pub fn transpose(x: Tensor, dim0: Int, dim1: Int) -> Tensor ::
    return enkai_tensor_transpose(x, dim0, dim1)
::

pub fn concat(handles, dim: Int) -> Tensor ::
    return enkai_tensor_concat(json.stringify(handles), dim)
::

pub fn sum(x: Tensor, dim: Int, keepdim: Bool) -> Tensor ::
    let keep := 0
    if keepdim ::
        keep := 1
    ::
    return enkai_tensor_sum(x, dim, keep)
::

pub fn mean(x: Tensor, dim: Int, keepdim: Bool) -> Tensor ::
    let keep := 0
    if keepdim ::
        keep := 1
    ::
    return enkai_tensor_mean(x, dim, keep)
::

pub fn softmax(x: Tensor, dim: Int) -> Tensor ::
    return enkai_tensor_softmax(x, dim)
::

pub fn masked_softmax(x: Tensor, mask: Tensor, dim: Int, mask_type: Int) -> Tensor ::
    return enkai_tensor_masked_softmax(x, mask, dim, mask_type)
::

pub fn masked_softmax_backward(grad_output: Tensor, output: Tensor, mask: Tensor, dim: Int) -> Tensor ::
    return enkai_tensor_masked_softmax_backward(grad_output, output, mask, dim)
::

pub fn relu(x: Tensor) -> Tensor ::
    return enkai_tensor_relu(x)
::

pub fn sigmoid(x: Tensor) -> Tensor ::
    return enkai_tensor_sigmoid(x)
::

pub fn dropout(x: Tensor, p: Float, train: Bool) -> Tensor ::
    let train_flag := 0
    if train ::
        train_flag := 1
    ::
    return enkai_tensor_dropout(x, p, train_flag)
::

pub fn slice(x: Tensor, dim: Int, start: Int, end: Int, step: Int) -> Tensor ::
    return enkai_tensor_slice(x, dim, start, end, step)
::

pub fn view(x: Tensor, shape) -> Tensor ::
    return enkai_tensor_view(x, json.stringify(shape))
::

pub fn layernorm(x: Tensor, w: Tensor, b: Tensor, eps: Float) -> Tensor ::
    return enkai_tensor_layernorm(x, w, b, eps)
::

pub fn layernorm_backward(x: Tensor, w: Tensor, b: Tensor, eps: Float, grad_out: Tensor) ::
    let raw := enkai_tensor_layernorm_backward(x, w, b, eps, grad_out)
    let handles := json.parse(raw)
    let out := json.parse("{\"dx\":0,\"dw\":0,\"db\":0}")
    out.dx := handles[0]
    out.dw := handles[1]
    out.db := handles[2]
    return out
::

pub fn embedding(w: Tensor, ids: Tensor) -> Tensor ::
    return enkai_tensor_embedding(w, ids)
::

pub fn linear(x: Tensor, w: Tensor, b: Tensor) -> Tensor ::
    return enkai_tensor_linear(x, w, b)
::

pub fn gelu(x: Tensor) -> Tensor ::
    return enkai_tensor_gelu(x)
::

pub fn to_device(x: Tensor, device: Device) -> Tensor ::
    return enkai_tensor_to_device(x, device)
::

pub fn to_dtype(x: Tensor, dtype: DType) -> Tensor ::
    return enkai_tensor_to_dtype(x, dtype)
::

pub fn shape(x: Int) ::
    let raw := enkai_tensor_shape(x)
    return json.parse(raw)
::

pub fn cross_entropy(logits: Tensor, targets: Tensor) -> Tensor ::
    return enkai_tensor_cross_entropy(logits, targets)
::

pub fn backward(loss: Tensor) ::
    return enkai_tensor_backward(loss)
::

pub fn require_grad(x: Tensor) -> Tensor ::
    return enkai_tensor_require_grad(x)
::

pub fn zero_grad(x: Tensor) -> Int ::
    return enkai_tensor_zero_grad(x)
::

pub fn zero_grad_multi(handles) -> Int ::
    return enkai_tensor_zero_grad_multi(json.stringify(handles))
::

pub fn adamw_step(param: Tensor, grad: Tensor, state: OptimizerState, lr: Float, beta1: Float, beta2: Float, eps: Float, wd: Float) -> OptimizerState ::
    return enkai_tensor_adamw_step(param, grad, state, lr, beta1, beta2, eps, wd)
::

pub fn adamw_step_multi(params, grads, state: OptimizerState, lr: Float, beta1: Float, beta2: Float, eps: Float, wd: Float) -> OptimizerState ::
    return enkai_tensor_adamw_step_multi(json.stringify(params), json.stringify(grads), state, lr, beta1, beta2, eps, wd)
::

pub fn param_group(params, grads) ::
    let group := json.parse("{\"params\":[],\"grads\":[]}")
    group.params := params
    group.grads := grads
    return group
::

pub fn param_group_step(group, state: OptimizerState, lr: Float, beta1: Float, beta2: Float, eps: Float, wd: Float) -> OptimizerState ::
    return adamw_step_multi(group.params, group.grads, state, lr, beta1, beta2, eps, wd)
::

pub fn save_sharded(dir: String, param: Tensor, opt_state: OptimizerState, meta) ::
    return enkai_tensor_save_sharded(dir, param, opt_state, json.stringify(meta))
::

pub fn save_sharded_multi(dir: String, params, opt_state: OptimizerState, meta) ::
    return enkai_tensor_save_sharded_multi(dir, json.stringify(params), opt_state, json.stringify(meta))
::

pub fn load_sharded(dir: String) ::
    let param := enkai_tensor_load_sharded(dir)
    let opt_state := enkai_tensor_load_sharded_opt(dir)
    let meta_raw := enkai_tensor_load_sharded_meta(dir)
    let meta := json.parse(meta_raw)
    let out := json.parse("{\"param\":0,\"opt_state\":0}")
    out.param := param
    out.opt_state := opt_state
    out.meta := meta
    return out
::

pub fn load_sharded_multi(dir: String) ::
    let params_raw := enkai_tensor_load_sharded_params(dir)
    let params := json.parse(params_raw)
    let opt_state := enkai_tensor_load_sharded_opt_multi(dir, json.stringify(params))
    let meta_raw := enkai_tensor_load_sharded_meta(dir)
    let meta := json.parse(meta_raw)
    let out := json.parse("{\"params\":0,\"opt_state\":0}")
    out.params := params
    out.opt_state := opt_state
    out.meta := meta
    return out
::

pub fn free(handle: Int) ::
    return enkai_tensor_free(handle)
::

pub fn free_device(handle: Int) ::
    return enkai_tensor_device_free(handle)
::


