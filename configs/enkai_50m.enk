fn main() ::
    return json.parse("{\"config_version\":1,\"backend\":\"native\",\"dataset_path\":\"data/train.txt\",\"eval_dataset_path\":\"data/eval.txt\",\"checkpoint_dir\":\"checkpoints/enkai_50m\",\"seq_len\":1024,\"batch_size\":8,\"lr\":0.0003,\"max_steps\":1000000,\"save_every\":500,\"log_every\":50,\"eval_steps\":50,\"drop_remainder\":true,\"add_eos\":true,\"pad_id\":0,\"keep_last\":3,\"model\":{\"vocab_size\":32000,\"hidden_size\":768,\"d_model\":768,\"n_layers\":8,\"n_heads\":12,\"device\":\"cuda:0\",\"dtype\":\"fp32\"},\"optim\":{\"lr\":0.0003,\"beta1\":0.9,\"beta2\":0.999,\"eps\":0.00000001,\"weight_decay\":0.01},\"tokenizer_train\":{\"path\":\"data/train.txt\",\"vocab_size\":32000,\"save_path\":\"checkpoints/enkai_50m/tokenizer.json\"}}")
::
